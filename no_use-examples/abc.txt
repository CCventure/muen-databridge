Bu benim bir onceki django uygulamam ayni proje icinde benzer bir uygulama yazacagiz oncelikle bunu oku:


admin:
from django.contrib import admin
from .models import ProductListSource, ProductUrl

class ProductUrlInline(admin.TabularInline):
    model = ProductUrl
    extra = 0
    readonly_fields = ('url', 'change_frequency', 'priority', 'lcw_last_modification', 'system_last_modification')
    can_delete = False
    show_change_link = True
    
    def has_add_permission(self, request, obj=None):
        return False

@admin.register(ProductListSource)
class ProductListSourceAdmin(admin.ModelAdmin):
    list_display = ('url', 'last_modification', 'last_fetch')
    inlines = [ProductUrlInline]
    
    def get_urls(self):
        from django.urls import path
        urls = super().get_urls()
        from .views import update_product_list
        custom_urls = [
            path('update-product-list/', update_product_list, name='update_product_list'),
        ]
        return custom_urls + urls

@admin.register(ProductUrl)
class ProductUrlAdmin(admin.ModelAdmin):
    list_display = ('url', 'source', 'change_frequency', 'priority', 'lcw_last_modification', 'system_last_modification')
    list_filter = ('source', 'change_frequency', 'priority', 'lcw_last_modification')
    search_fields = ('url',)
    readonly_fields = ('system_last_modification',)


model:
from django.db import models

class ProductListSource(models.Model):
    url = models.URLField(max_length=255, default="")
    last_modification = models.DateField(null=True, blank=True)
    last_fetch = models.DateTimeField(null=True, blank=True)

    def __str__(self):
        return self.url
    
    class Meta:
        verbose_name = "Product List Source"
        verbose_name_plural = "Product List Sources"
        
class ProductUrl(models.Model):
    source = models.ForeignKey(ProductListSource, on_delete=models.CASCADE, related_name='product_urls')
    url = models.URLField(max_length=255)
    change_frequency = models.CharField(max_length=50, blank=True, null=True)
    priority = models.CharField(max_length=10, blank=True, null=True)
    lcw_last_modification = models.DateField(null=True, blank=True)
    system_last_modification = models.DateTimeField(auto_now=True)

    def __str__(self):
        return self.url

    class Meta:
        verbose_name = "Product URL"
        verbose_name_plural = "Product URLs"
        unique_together = ('source', 'url')

apps:
from django.apps import AppConfig
from django.db.models.signals import post_migrate
from datetime import datetime

class ProductSitemapConfig(AppConfig):
    default_auto_field = 'django.db.models.BigAutoField'
    name = 'scraper_apps.lcwaikiki.product_list_api'
    
    def ready(self):
        # Uygulama başladığında çalışacak kodlar
        from django.conf import settings
        import django_apscheduler.jobstores
        from apscheduler.schedulers.background import BackgroundScheduler
        from apscheduler.triggers.interval import IntervalTrigger
        
        try:
            # Schedule task
            scheduler = BackgroundScheduler()
            from scraper_apps.lcwaikiki.product_list_api.tasks import fetch_product_list_data
            
            scheduler.add_job(
                fetch_product_list_data,
                trigger=IntervalTrigger(hours=16),
                id='fetch_product_list_job',
                replace_existing=True
                # next_run_time=datetime.now()
            )
            
            post_migrate.connect(self.initial_fetch, sender=self)
            
            scheduler.start()
        except Exception as e:
            print(f"Scheduler başlatılırken hata: {str(e)}")
    
    def initial_fetch(self, sender, **kwargs):
        from scraper_apps.lcwaikiki.product_list_api.tasks import fetch_product_list_data
        

        fetch_product_list_data()


serializers:
from rest_framework import serializers
from scraper_apps.lcwaikiki.product_list_api.models import ProductUrl

class ProductUrlSerializer(serializers.ModelSerializer):
    class Meta:
        model = ProductUrl
        fields = ('id', 'url', 'source', 'change_frequency', 'priority', 'lcw_last_modification', 'system_last_modification')

class ProductUrlListSerializer(serializers.ModelSerializer):
    source = serializers.StringRelatedField()
    
    class Meta:
        model = ProductUrl
        fields = ('id', 'url', 'source', 'change_frequency', 'priority', 'lcw_last_modification', 'system_last_modification')
    
    def to_representation(self, instance):
        data = super().to_representation(instance)
        return {
            "id": data['id'],
            "url": data['url'],
            "source": data['source'],
            "change_frequency": data['change_frequency'],  # Hata düzeltilmiş hali
            "priority": data['priority'],
            "lcw_last_modification": data['lcw_last_modification'],
            "system_last_modification": data['system_last_modification'],
        }

class FilteredProductUrlSerializer(serializers.ModelSerializer):
    class Meta:
        model = ProductUrl
        fields = ('id', 'url', 'change_frequency', 'priority', 'lcw_last_modification', 'system_last_modification')
    
    def to_representation(self, instance):
        data = super().to_representation(instance)
        return {
            "id": data['id'],
            "url": data['url'],
            "change_frequency": data['change_frequency'],
            "priority": data['priority'],
            "lcw_last_modification": data['lcw_last_modification'],
            "system_last_modification": data['system_last_modification'],
        }

class OnlyUrlsSerializer(serializers.ModelSerializer):
    class Meta:
        model = ProductUrl
        fields = ('id', 'url',)
    
    def to_representation(self, instance):
        return {
            "id": instance.id,
            "url": instance.url
        }


tasks:
import requests
import xml.etree.ElementTree as ET
import random
import logging
import time
from datetime import datetime
from django.utils import timezone
from django.conf import settings
from scraper_apps.lcwaikiki.product_sitemap_api.models import SitemapUrl
from scraper_apps.lcwaikiki.product_list_api.models import ProductListSource, ProductUrl

logger = logging.getLogger(__name__)

class ProductListScraper:
    """LCWaikiki product XML list scraper with improved proxy and user agent handling"""
    
    # Define a list of common user agents
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15',
        'Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/119.0',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (iPad; CPU OS 16_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Mobile/15E148 Safari/604.1'
    ]
    
    def __init__(self):
        self.session = requests.Session()
        self.max_retries = 5
        self.retry_delay = 5
        
    def _get_random_proxy(self):
        """Get a random proxy from settings"""
        if not settings.PROXY_LIST:
            return None
        return random.choice(settings.PROXY_LIST)
    
    def _get_headers(self):
        """Generate request headers with random user agent"""
        user_agent = random.choice(self.USER_AGENTS)
        return {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Referer': 'https://www.lcw.com/',
            'User-Agent': user_agent,
        }
        
    def fetch(self, url, max_proxy_attempts=None):
        """Fetch URL content with retry and proxy rotation logic"""
        if max_proxy_attempts is None:
            max_proxy_attempts = len(settings.PROXY_LIST) if settings.PROXY_LIST else 1
            
        proxy_attempts = 0
        proxies_tried = set()
        
        while proxy_attempts < max_proxy_attempts:
            # Get a proxy that hasn't been tried yet if possible
            available_proxies = [p for p in settings.PROXY_LIST if p not in proxies_tried] if settings.PROXY_LIST else [None]
            if not available_proxies:
                logger.warning("All proxies have been tried without success")
                break
                
            proxy = random.choice(available_proxies)
            proxies = {"http": proxy, "https": proxy} if proxy else None
            
            if proxy:
                proxies_tried.add(proxy)
            
            # Update headers with new random user agent for each attempt
            self.session.headers.update(self._get_headers())
            
            logger.info(f"Proxy attempt {proxy_attempts + 1}/{max_proxy_attempts}: {proxy}")
            
            # Try up to max_retries times with this proxy
            for attempt in range(1, self.max_retries + 1):
                try:
                    response = self.session.get(
                        url, 
                        proxies=proxies, 
                        timeout=30,
                        allow_redirects=True,
                        verify=True
                    )
                    
                    if response.status_code == 200:
                        logger.info(f"Successfully fetched {url}")
                        return response.content
                    
                    elif response.status_code == 403:
                        logger.warning(f"Access denied (403) with proxy {proxy}, attempt {attempt}")
                        if attempt == self.max_retries:
                            break  # Try next proxy
                        time.sleep(self.retry_delay * attempt)  # Exponential backoff
                        
                    else:
                        logger.warning(f"HTTP {response.status_code} with proxy {proxy}, attempt {attempt}")
                        response.raise_for_status()
                        
                except requests.exceptions.RequestException as e:
                    logger.warning(f"Request error with proxy {proxy}, attempt {attempt}: {str(e)}")
                    if attempt == self.max_retries:
                        break  # Try next proxy
                    time.sleep(self.retry_delay * attempt)  # Exponential backoff
            
            proxy_attempts += 1
        
        logger.error("All proxy attempts failed")
        return None

    def parse_product_list(self, content):
        """Parse product list XML content"""
        if not content:
            return []
            
        try:
            # Parse XML
            root = ET.fromstring(content)
            
            # Extract product URLs
            product_entries = []
            for url_elem in root.findall(".//url"):
                loc_elem = url_elem.find("loc")
                lastmod_elem = url_elem.find("lastmod")
                changefreq_elem = url_elem.find("changefreq")
                priority_elem = url_elem.find("priority")
                
                if loc_elem is not None and loc_elem.text:
                    url = loc_elem.text.strip()
                    lastmod = None
                    changefreq = None
                    priority = None
                    
                    if lastmod_elem is not None and lastmod_elem.text:
                        try:
                            lastmod_text = lastmod_elem.text.strip()
                            # Parse the date
                            lastmod = datetime.strptime(lastmod_text, "%Y-%m-%d").date()
                        except ValueError:
                            logger.warning(f"Invalid date format: {lastmod_elem.text}")
                    
                    if changefreq_elem is not None and changefreq_elem.text:
                        changefreq = changefreq_elem.text.strip()
                        
                    if priority_elem is not None and priority_elem.text:
                        priority = priority_elem.text.strip()
                    
                    product_entries.append((url, lastmod, changefreq, priority))
            
            return product_entries
            
        except ET.ParseError as e:
            logger.error(f"XML parse error: {str(e)}")
            return []

def fetch_sitemap_urls():
    """Fetch URLs from sitemap API"""
    try:
        sitemap_api_url = f"/api/{settings.CURRENTLY_API_VERSION}/lcwaikiki/product-sitemap/"
        # Get base URL from settings or use a default
        base_url = getattr(settings, 'BASE_API_URL', 'http://localhost:8000')
        
        full_url = f"{base_url.rstrip('/')}{sitemap_api_url}"
        logger.info(f"Fetching sitemap URLs from: {full_url}")
        
        response = requests.get(full_url)
        if response.status_code == 200:
            data = response.json()
            return data.get('urls', [])
        else:
            logger.error(f"Failed to fetch sitemap URLs: HTTP {response.status_code}")
            return []
    except Exception as e:
        logger.error(f"Error fetching sitemap URLs: {str(e)}")
        return []

def fetch_product_list_data():
    """Main function to fetch and update product list data"""
    scraper = ProductListScraper()
    sitemap_urls = fetch_sitemap_urls()
    
    if not sitemap_urls:
        logger.error("No sitemap URLs available")
        return False
    
    try:
        total_products = 0
        
        for sitemap_url_data in sitemap_urls:
            url = sitemap_url_data.get("url")
            last_modification = sitemap_url_data.get("last_modification")
            
            if not url:
                continue
                
            logger.info(f"Processing sitemap URL: {url}")
            
            # Convert last_modification string to date object if it exists
            last_mod_date = None
            if last_modification:
                try:
                    last_mod_date = datetime.strptime(last_modification, "%Y-%m-%d").date()
                except ValueError:
                    logger.warning(f"Invalid date format: {last_modification}")
            
            # Create or update source
            source, created = ProductListSource.objects.update_or_create(
                url=url,
                defaults={
                    'last_modification': last_mod_date
                }
            )
            
            # Fetch product list content
            content = scraper.fetch(url)
            if not content:
                logger.error(f"Failed to fetch product list content from {url}")
                continue
            
            # Parse and store product entries
            product_entries = scraper.parse_product_list(content)
            if not product_entries:
                logger.warning(f"No product entries found in {url}")
                continue
            
            # Create new product URLs
            for product_url, lastmod, changefreq, priority in product_entries:
                try:
                    ProductUrl.objects.update_or_create(
                        source=source,
                        url=product_url,
                        defaults={
                            'lcw_last_modification': lastmod,
                            'change_frequency': changefreq,
                            'priority': priority
                        }
                    )
                    total_products += 1
                except Exception as e:
                    logger.error(f"Error creating product URL: {str(e)}")
            
            # Update last fetch time
            source.last_fetch = timezone.now()
            source.save()
            
            logger.info(f"Successfully processed {url}: {len(product_entries)} products stored")
        
        logger.info(f"Total products processed: {total_products}")
        return True
        
    except Exception as e:
        logger.error(f"Error updating product list: {str(e)}", exc_info=True)
        return False


urls:
from django.urls import path
from scraper_apps.lcwaikiki.product_list_api.views import (
    product_list_api, 
    product_list_only_urls_api
)

urlpatterns = [
    path('full/', product_list_api, name='product_list_api'),
    path('only-urls/', product_list_only_urls_api, name='product_list_only_urls_api'),
]


views:
from django.shortcuts import render, redirect
from django.http import JsonResponse
from django.contrib import messages
from django.urls import reverse
from rest_framework.decorators import api_view
from rest_framework.response import Response
from rest_framework import status
from scraper_apps.lcwaikiki.product_list_api.models import ProductListSource, ProductUrl
from scraper_apps.lcwaikiki.product_list_api.serializers import (
    ProductUrlListSerializer, FilteredProductUrlSerializer, OnlyUrlsSerializer
)
from scraper_apps.lcwaikiki.product_list_api.tasks import fetch_product_list_data
import logging

logger = logging.getLogger(__name__)

def update_product_list(request):
    try:
        result = fetch_product_list_data()
        
        if result:
            messages.success(request, "Ürün listesi başarıyla güncellendi.")
        else:
            messages.warning(request, "Ürün listesi güncellenirken bazı sorunlar oluştu. Lütfen loglara bakın.")
    except Exception as e:
        logger.error(f"Ürün listesi güncellenirken hata oluştu: {str(e)}")
        messages.error(request, f"Ürün listesi güncellenirken hata oluştu: {str(e)}")
    
    # Admin product list sayfasına yönlendir
    return redirect(reverse('admin:product_list_api_productlistsource_changelist'))

@api_view(['GET'])
def product_list_api(request):
    try:
        # Source filtresi kontrolü
        source_filter = request.query_params.get('source', None)
        
        if source_filter:
            try:
                # Sayısal ID ile filtreleme deneyin
                if source_filter.isdigit():
                    source_id = int(source_filter)
                    # Önce ID ile arayın, bulunamazsa URL ile arayın
                    try:
                        source = ProductListSource.objects.get(id=source_id)
                    except ProductListSource.DoesNotExist:
                        # URL içinde ID geçiyorsa onu bulmaya çalışın
                        sources = ProductListSource.objects.filter(url__contains=f"Products-{source_id}")
                        if sources.exists():
                            source = sources.first()
                        else:
                            return Response({"error": f"Source ID {source_id} bulunamadı"}, 
                                            status=status.HTTP_404_NOT_FOUND)
                else:
                    # URL olarak arayın
                    source = ProductListSource.objects.get(url=source_filter)
                    
                products = ProductUrl.objects.filter(source=source)
                serializer = FilteredProductUrlSerializer(products, many=True)
                return Response({"product_adress": serializer.data})
            except ProductListSource.DoesNotExist:
                return Response({"error": f"Source bulunamadı: {source_filter}"}, 
                                status=status.HTTP_404_NOT_FOUND)
        else:
            # Tüm ürünleri listele
            products = ProductUrl.objects.all()
            serializer = ProductUrlListSerializer(products, many=True)
            return Response({"product_adress": serializer.data})
    except Exception as e:
        logger.error(f"API hata: {str(e)}")
        return Response({"error": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@api_view(['GET'])
def product_list_only_urls_api(request):
    try:
        # Source filtresi kontrolü
        source_filter = request.query_params.get('source', None)
        
        if source_filter:
            try:
                # Sayısal ID ile filtreleme deneyin
                if source_filter.isdigit():
                    source_id = int(source_filter)
                    # Önce ID ile arayın, bulunamazsa URL ile arayın
                    try:
                        source = ProductListSource.objects.get(id=source_id)
                    except ProductListSource.DoesNotExist:
                        # URL içinde ID geçiyorsa onu bulmaya çalışın
                        sources = ProductListSource.objects.filter(url__contains=f"Products-{source_id}")
                        if sources.exists():
                            source = sources.first()
                        else:
                            return Response({"error": f"Source ID {source_id} bulunamadı"}, 
                                            status=status.HTTP_404_NOT_FOUND)
                else:
                    # URL olarak arayın
                    source = ProductListSource.objects.get(url=source_filter)
                    
                products = ProductUrl.objects.filter(source=source)
            except ProductListSource.DoesNotExist:
                return Response({"error": f"Source bulunamadı: {source_filter}"}, 
                                status=status.HTTP_404_NOT_FOUND)
        else:
            # Tüm ürünleri listele
            products = ProductUrl.objects.all()
            
        serializer = OnlyUrlsSerializer(products, many=True)
        return Response({"product_adress": serializer.data})
    except Exception as e:
        logger.error(f"API hata: {str(e)}")
        return Response({"error": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)





SImdi bizim yazacagigimiz yeni uygulamanin adi ise product_api olacak.
Bu uygulamanin isi su olacak bu biraz detayli o yuzden iyice oku.

Simdi uygulama ilk olarak /api/{setting.CURRENT_API_VERSION}/lcwaikiki/product-sitemap/ adresine istek atacak buradan tum sitemaplari alacak ve io da tutacak dursun diye gerecekecek tek tek cunki.

{
    "source": "https://www.lcw.com/sitemap/api/FeedXml/TR/product_sitemap.xml",
    "urls": [
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-1/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-2/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-3/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-4/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-5/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-6/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-7/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-8/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-9/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-10/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-11/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-12/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-13/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-14/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-15/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-16/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-17/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-18/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-19/xml",
            "last_modification": "2025-03-27"
        },
        {
            "url": "https://api.lcwaikiki.com/feed/service/api/TR/Products-20/xml",
            "last_modification": "2025-03-27"
        }
    ]
}

icerik su sekilde burdaki urlslerdeki urlleri alacak hepsi io da tutacak sonra sirayla kullanacak ilk olarak:
http://127.0.0.1:8000/api/{settings.CURRENTY_API_VERSION}/lcwaikiki/product-list/only-urls/?source=https://api.lcwaikiki.com/feed/service/api/TR/Products-1/xml adresine gidecek icerikte bulunan tum urlleri alacak ve tek tek request atacak onlara.
http://127.0.0.1:8000/api/{settings.CURRENTY_API_VERSION}/lcwaikiki/product-list/only-urls/?source=https://api.lcwaikiki.com/feed/service/api/TR/Products-1/xml icerigi ornek:
(her birinde 13 -15 bin tane url var uygun sekilde handle et yani)
{
    "product_adress": [
        {
            "id": 1,
            "url": "https://www.lcw.com/kravat-ve-mendil-kahverengi-o-682249"
        },
        {
            "id": 2,
            "url": "https://www.lcw.com/slim-fit-dik-yaka-kapusonlu-reflektorlu-erkek-sisme-mont-gri-o-1428165"
        },
        {
            "id": 3,
            "url": "https://www.lcw.com/standart-kalip-roller-erkek-sort-lacivert-o-1584696"
        },
        {
            "id": 4,
            "url": "https://www.lcw.com/standart-kalip-roller-erkek-sort-siyah-o-1480645"
        },
        {
            "id": 5,
            "url": "https://www.lcw.com/standart-kalip-roller-erkek-sort-gri-o-1480646"
        },
        {
            "id": 6,
            "url": "https://www.lcw.com/standart-kalip-beli-lastikli-erkek-sort-lacivert-o-1481530"
        },
        {
            "id": 7,
            "url": "https://www.lcw.com/standart-kalip-beli-lastikli-erkek-sort-haki-o-1756574"
        },
]
}
Bu urller bittikten sonra sira diger urllere gelecek yani http://127.0.0.1:8000/api/{settings.CURRENTY_API_VERSION}/lcwaikiki/product-list/only-urls/?source=https://api.lcwaikiki.com/feed/service/api/TR/Products-2/xml boylece tum hepsi bitene kadar.

Bu islemi hizli tutmak icin yani bu ruller fazla dedigim gibi 200k + her birine istek atmak icin 5 thread kullanilacak.
Her thread farkli proxy kullanacak basarisiz olma durumunda 403 alma durumunda istek tekrarlanacak.

urllere istek atarken soyle ki ben daha nce scrapy ile yazdim basit birsey ama bunu duzenle koda uygumlu halw gewrie bu kotu:
import scrapy
import json
import re
from urllib.parse import urlparse
from typing import Dict, Any, Optional
from itemadapter import ItemAdapter
from datetime import datetime

class LcGetProductsDataSpider(scrapy.Spider):
    name = 'lc_get_product'
    
    custom_settings = {
        'FEEDS': {
            'lc_products_data.jsonl': {
                'format': 'jsonlines',
                'overwrite': True,
            }
        },
        'CONCURRENT_REQUESTS': 4,
        'DOWNLOAD_DELAY': 1,
        'RETRY_TIMES': 3
    }

    DEFAULT_CITY_ID = "865"
    INVENTORY_API_URL = "https://www.lcw.com/tr-TR/TR/ajax/Model/GetStoreInventoryMultiple"

    start_urls = [
        'https://www.lcw.com/beli-lastikli-kadin-esofman-alti-siyah-o-4423280',
    ]

    def parse(self, response):
        meta_tags = self.extract_meta_tags(response)
        
        json_data = self.extract_json_data(response)

        if json_data is None:
            self.logger.error(f"No JSON data found for {response.url}")
            return

        product_sizes = json_data.get('ProductSizes', [])
        
        product = {
            "url": response.url,
            "title": json_data.get('PageTitle', ''),
            "category": json_data.get('CategoryName', ''),
            "color": json_data.get('Color', ''),
            "price": {
                "price": json_data.get('ProductPrices', {}).get('Price', ''),
                "discount_ratio": json_data.get('ProductPrices', {}).get('DiscountRatio', 0)
            },
            "in_stock": json_data.get('IsInStock', False),
            "sizes": [],
            "images": [
                img_url for pic in json_data.get('Pictures', [])
                for img_size in ['ExtraMedium600', 'ExtraMedium800', 'MediumImage', 'SmallImage']
                if (img_url := pic.get(img_size))
            ],
            "timestamp": datetime.now().isoformat(),
            "status": "success",
            "pending_size_requests": 0  # Track how many size requests we're waiting for
        }
        
        # Count how many sizes need inventory checks
        sizes_with_stock = [size for size in product_sizes if size.get('Stock', 0) > 0]
        product['pending_size_requests'] = len(sizes_with_stock)
        
        # If no sizes need inventory checks, yield immediately
        if not sizes_with_stock:
            for size in product_sizes:
                product['sizes'].append({
                    "size_name": size.get('Size', {}).get('Value', ''),
                    "size_id": size.get('Size', {}).get('SizeId', ''),
                    "size_general_stock": size.get('Stock', 0),
                    "product_option_size_reference": size.get('UrunOptionSizeRef', 0),
                    "barcode_list": size.get('BarcodeList', [])
                })
            yield product
            return
        
        # Process each size and request inventory if needed
        for size in product_sizes:
            size_info = {
                "size_name": size.get('Size', {}).get('Value', ''),
                "size_id": size.get('Size', {}).get('SizeId', ''),
                "size_general_stock": size.get('Stock', 0),
                "product_option_size_reference": size.get('UrunOptionSizeRef', 0),
                "barcode_list": size.get('BarcodeList', [])
            }
            
            if size_info['size_general_stock'] > 0:
                yield scrapy.Request(
                    url=self.INVENTORY_API_URL,
                    method='POST',
                    headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                        'Content-Type': 'application/json',
                        'Referer': response.url
                    },
                    body=json.dumps({
                        "cityId": self.DEFAULT_CITY_ID,
                        "countyIds": [],
                        "urunOptionSizeRef": str(size_info['product_option_size_reference'])
                    }),
                    callback=self.parse_inventory,
                    meta={
                        'product': product,
                        'size_info': size_info,
                        'original_url': response.url,
                        'dont_merge_cookies': True,
                        'proxy': True,
                    }
                )
            else:
                # If no stock, just add the size info without city data
                product['sizes'].append(size_info)
                self.logger.debug(f"Size {size_info['size_name']} has no stock, skipping inventory check")

    def parse_inventory(self, response):
        product = response.meta['product']
        size_info = response.meta['size_info']
        original_url = response.meta['original_url']
        
        try:
            inventory_data = json.loads(response.text)
        except json.JSONDecodeError:
            self.logger.error(f"Failed to parse inventory JSON for {original_url}")
            inventory_data = {"storeInventoryInfos": []}
        
        city_stock = {}
        total_city_stock = 0
        
        for store in inventory_data.get('storeInventoryInfos', []):
            city_id = str(store.get('StoreCityId'))
            city_name = store.get('StoreCityName')
            
            if city_id not in city_stock:
                city_stock[city_id] = {
                    "name": city_name,
                    "stock": 0,
                    "stores": []
                }
            
            store_info = {
                "store_code": store.get('StoreCode', ''),
                "store_name": store.get('StoreName', ''),
                "store_address": {
                    "location_with_words": store.get('Address', ''),
                    "location_with_coordinants": [
                        store.get('Lattitude', ''),
                        store.get('Longitude', '')
                    ]
                },
                "store_phone": store.get('StorePhone', ''),
                "store_county": store.get('StoreCountyName', ''),
                "stock": store.get('Quantity', 0)
            }
            
            city_stock[city_id]['stores'].append(store_info)
            city_stock[city_id]['stock'] += store.get('Quantity', 0)
            total_city_stock += store.get('Quantity', 0)
        
        # Add city stock info to the size
        size_info['size_city_stock'] = city_stock
        size_info['size_general_stock'] = total_city_stock  # Update with actual total from API
        
        # Add the size to the product
        product['sizes'].append(size_info)
        
        # Decrement our pending requests counter
        product['pending_size_requests'] -= 1
        
        # Check if we've processed all inventory requests
        if product['pending_size_requests'] == 0:
            # Sort sizes by name for consistent output
            product['sizes'].sort(key=lambda x: x.get('size_name', ''))
            self.logger.debug(f"Yielding product with inventory data: {product}")
            yield product

    def extract_meta_tags(self, response) -> Dict[str, str]:
        """Extract all meta tags"""
        meta_tags = {}
        for meta in response.xpath('//meta'):
            name = meta.xpath('@name').get()
            property_ = meta.xpath('@property').get()
            content = meta.xpath('@content').get()
            
            if name:
                meta_tags[f'meta_name_{name}'] = content
            if property_:
                meta_tags[f'meta_property_{property_}'] = content
        
        return meta_tags

    def extract_json_data(self, response) -> Optional[Dict[str, Any]]:
        script_tags = response.xpath('//script[contains(text(), "cartOperationViewModel")]/text()').getall()
        
        for script in script_tags:
            try:
                pattern = r'cartOperationViewModel\s*=\s*({.*?});'
                match = re.search(pattern, script, re.DOTALL)
                if match:
                    json_str = match.group(1).strip()
                    json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)
                    json_str = re.sub(r',\s*}', '}', json_str)
                    json_str = re.sub(r',\s*]', ']', json_str)
                    
                    return json.loads(json_str)
            except Exception as e:
                self.logger.error(f"JSON parsing error in {response.url}: {str(e)}")
                continue
        
        self.logger.warning(f"No valid JSON data found in {response.url}")
        return None



API soyle olacak ornek olarak bu asdece ornek yani
{
    "product_list": [
        {
            "url": "https://www.lcw.com/beli-lastikli-kadin-esofman-alti-siyah-o-4423280",
            "title": "Beli Lastikli Kadın Eşofman Altı - Siyah",
            "category": "Kadın > Alt Giyim > Eşofman",
            "color": "Siyah",
            "price": {
              "price": 299.99,
              "discount_ratio": 0.15
            },
            "in_stock": true,
            "sizes": [
              {
                "size_name": "S",
                "size_id": "123456",
                "size_general_stock": 5,
                "product_option_size_reference": "987654",
                "barcode_list": ["1234567890123"],
                "size_city_stock": {
                  "34": {
                    "name": "İstanbul",
                    "stock": 2,
                    "stores": [
                      {
                        "store_code": "IST001",
                        "store_name": "LCW Cevahir",
                        "store_address": {
                          "location_with_words": "Şişli, Cevahir AVM",
                          "location_with_coordinants": ["41.0659", "28.9871"]
                        },
                        "store_phone": "0212 123 4567",
                        "store_county": "Şişli",
                        "stock": 2
                      }
                    ]
                  },
                  "35": {
                    "name": "İzmir",
                    "stock": 3,
                    "stores": [
                      {
                        "store_code": "IZM001",
                        "store_name": "LCW Bornova",
                        "store_address": {
                          "location_with_words": "Bornova Forum AVM",
                          "location_with_coordinants": ["38.4623", "27.2228"]
                        },
                        "store_phone": "0232 765 4321",
                        "store_county": "Bornova",
                        "stock": 3
                      }
                    ]
                  }
                }
              },
              {
                "size_name": "M",
                "size_id": "123457",
                "size_general_stock": 0,
                "product_option_size_reference": "987655",
                "barcode_list": ["1234567890124"]
              }
            ],
            "images": [
              "https://www.lcw.com/path/to/image1.jpg",
              "https://www.lcw.com/path/to/image2.jpg"
            ],
            "timestamp": "2025-04-04T16:45:23.457843",
            "status": "success",
            "pending_size_requests": 0
          },
          {
            "url": "https://www.lcw.com/beli-lastikli-kadin-esofman-alti-siyah-o-4423280",
            "title": "Beli Lastikli Kadın Eşofman Altı - Siyah",
            "category": "Kadın > Alt Giyim > Eşofman",
            "color": "Siyah",
            "price": {
              "price": 299.99,
              "discount_ratio": 0.15
            },
            "in_stock": true,
            "sizes": [
              {
                "size_name": "S",
                "size_id": "123456",
                "size_general_stock": 5,
                "product_option_size_reference": "987654",
                "barcode_list": ["1234567890123"],
                "size_city_stock": {
                  "34": {
                    "name": "İstanbul",
                    "stock": 2,
                    "stores": [
                      {
                        "store_code": "IST001",
                        "store_name": "LCW Cevahir",
                        "store_address": {
                          "location_with_words": "Şişli, Cevahir AVM",
                          "location_with_coordinants": ["41.0659", "28.9871"]
                        },
                        "store_phone": "0212 123 4567",
                        "store_county": "Şişli",
                        "stock": 2
                      }
                    ]
                  },
                  "35": {
                    "name": "İzmir",
                    "stock": 3,
                    "stores": [
                      {
                        "store_code": "IZM001",
                        "store_name": "LCW Bornova",
                        "store_address": {
                          "location_with_words": "Bornova Forum AVM",
                          "location_with_coordinants": ["38.4623", "27.2228"]
                        },
                        "store_phone": "0232 765 4321",
                        "store_county": "Bornova",
                        "stock": 3
                      }
                    ]
                  }
                }
              },
              {
                "size_name": "M",
                "size_id": "123457",
                "size_general_stock": 0,
                "product_option_size_reference": "987655",
                "barcode_list": ["1234567890124"]
              }
            ],
            "images": [
              "https://www.lcw.com/path/to/image1.jpg",
              "https://www.lcw.com/path/to/image2.jpg"
            ],
            "timestamp": "2025-04-04T16:45:23.457843",
            "status": "success",
            "pending_size_requests": 0
          }                    
    ]
}

burda api filtrelenebilir olsun istiyorum ornek olarak sehire gore filtreleme, birden fazla sehire gore filtreleme, linke gore filtreleme, stoka gore filtreleme, magazaya gore filtreleme, store phone,a gore filtreleme, store stocklarina gore filtereleme, olcuye gore filtreleme, belirli uruniu olcuye gore filtreleme, bunlarin neredeyse birconunu ayni anda kullanabilmek ve s gibi gelismis filtrelem istiyorum ayni sekilde admin paneld de bu sekilde olsun.